+++
title = "AI Summit Munich 2023"
image = "images/ev_tum_ai_summit/theme_photo.png"
summary = "Speaker: Muthasham Oblokulov | Oct 20-21th, 2023"
recording = "https://youtu.be/Ug5c92gxDu0?si=N0ynH37F3Aza0Gn3"
url = "/events/tumai-summit"
date = "2023-10-20"
+++

{{< youtube "Ug5c92gxDu0?si=N0ynH37F3Aza0Gn3" >}}


### About this event

How does model architecture, pre-training objective, the side of the dataset and parameter count affect model's linguistic abilities? They don't ðŸ¤¯. Or at least not as directly we thought.
Evaluation of generated text remains a significant issue. Recently-introduced model-based metrics have shown promising results compared to n-gram-based metrics like BLEU, yet they still suffer severe drawbacks (http://arxiv.org/abs/2205.10696).


### Slides

{{< gslides src="https://docs.google.com/presentation/d/e/2PACX-1vT6GIGbxgpFRmfjxrCXq2hAmbnHXVbxuwGChjR2Dd76WaLQId-TeRVE299fsizMBkw4vIorvy74P4pI/embed?start=false&loop=false&delayms=3000" >}}


### Speaker

![Muhtasham Oblokulov ><](/images/muhtasham-oblokulov.jpg)

**Muhtasham Oblokulov** is a Machine Learning Engineer at Munich Re. He is passionate about applied AI, specifically transfer learning in NLP. Apart from that, his experience lies in Brain Computer Interfaces, Anomaly and Out of Distribution detection, Synthetic Data Augmentation, and NLP for Low-Resource Languages. In May 2022, he co-founded Munich NLP with colleagues from TUM and LMU.