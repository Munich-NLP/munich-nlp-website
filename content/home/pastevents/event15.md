+++
title = "What learning algorithm is in-context learning?"
image = "/images/ev_nlp_in_con_len_ekin_akyurek/theme_photo.jpg"
summary = "Speaker: Ekin Aky端rek | Dec 22, 2022 17:00-18:30"
recording = ""
url = "/events/ekin-akyurek-in-context-learning"
date = "2022-12-22"
+++

<!--more-->

![What learning algorithm is in-context learning? ><](/images/ev_nlp_in_con_len_ekin_akyurek/theme_photo.jpg)

<!-- ### Location

[Join the Event!](https://discord.gg/Ny9PG36NRw?event=1054125834455236728) -->


### About this event

Ekin Aky端rek from MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) will explain his recent work including 

- The hypothesis that language models learn in-context by learning algorithms running internally.
- Novel way of proving that the Transformer architecture can learn in-context by internal gradient descent and ridge regression without any approximation to the architecture
- The empirical fact that the trained Transformers learn in-context very close to Bayes optimal algorithms, and do not learn in-context by gradient descent.
- Phase shifts between algorithmic behavior and scaling laws based model size vs problem dimension
- The distinction between behavior and computation.


### Speaker

![Ekin Akz端rek ><](https://www.ekinakyurek.me/assets/ekin.jpg)

**Ekin Akz端rek** is a computer science PhD student at the Massachusetts Institute of Technology (MIT). He studies artificial intelligence through natural language processing and machine learning and is advised by Jacob Andreas.

He works on improving sequence modeling for language processing and understanding.