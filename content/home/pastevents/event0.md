+++ 
title = "Life after BERT: What do Other Muppets Understand about Language?"
image = "Munich-NLP/munich-nlp.github.io/static/images/life-after.jpeg" 
summary = "Speaker: Vladislav Lialin | May 27, 2022" 
recording = "" 
url = "/events/life-after-bert" 
date = "2022-06-18" 
+++

<!--more-->


### About this event

How does model architecture, pre-training objective, the side of the dataset and parameter count affect model's linguistic abilities? They don't ðŸ¤¯. Or at least not as directly we thought.
Evaluation of generated text remains a significant issue. Recently introduced model-based metrics have shown promising results compared to n-gram-based metrics like BLEU, yet they still suffer severe drawbacks (http://arxiv.org/abs/2205.10696).


### Speaker

![Vladislav Lialin ><](/images/vlad.jpg)

Vladislav Lialin
I am a computer science PhD student at University of Massachusetts Lowell advised by Anna Rumshisky. My research areas areas include continual learning for large language models, multimodal learning and model analysis. In particular, I'm hyping large-scale models and think that every task is a language modeling task if you try hard enough. Currently interning at Amazon Alexa AI.