+++
title = "June Meetup"
image = "/images/ev_june24_meetup/theme_photo.jpeg"
summary = "Speaker: Haris Jabbar, Diego Miguel Lozano | June 20, 2024 18:00-22:00"
url = "/events/june-24-meetup"
date = "2024-06-20"
+++


### [RSVP](https://www.meetup.com/pydata-munchen/events/301469848/?isFirstPublish=true)


### Location
[QualityMinds GmbH](https://qualityminds.com/de/),
[Chiemgaustraße 116, 81549 München](https://www.google.com/maps/place//data=!4m2!3m1!1s0x479ddf1271af37d5:0x62053949af74d856?sa=X&ved=1t:8290&ictx=111)


#### Diego Miguel Lozano: _I'm Feeling Lucky: The Past, Present and Future of Search_
[KNOWRON](https://www.knowron.com/de) is a Munich-based startup that develops an AI-powered personal assistant to help deskless workers spend less time searching and more time finding. In this talk, we will cover the history of search with special emphasis on our own learnings in the last few years, in which new techniques such as Dense Passage Retrieval or Hybrid Search have greatly advanced the field of Information Retrieval.


#### Haris Jabbar: _MorphPiece: A Linguistic Tokenizer for Large Language Models_ [[Paper]](https://arxiv.org/abs/2307.07262)
Tokenization is a critical part of modern NLP pipelines. However, contemporary tokenizers for
Large Language Models are based on statistical analysis of text corpora, without much
consideration to the linguistic features. This work proposes a linguistically motivated
tokenization scheme, MorphPiece, which is based partly on morphological segmentation of the
underlying text. A GPT-style causal language model trained on this tokenizer (called
MorphGPT) shows comparable or superior performance on a variety of supervised and
unsupervised NLP tasks, compared to the OpenAI GPT-2 model. Specifically the model is
evaluated on language modeling tasks, zero-shot performance on GLUE Benchmark with
various prompt templates, massive text embedding benchmark (MTEB) for supervised and
unsupervised performance, and lastly with another morphological tokenization scheme (FLOTA,
Hoffmann et al., 2022). We find that the model trained on MorphPiece outperforms GPT-2 on
most evaluations, at times with considerable margin, despite being trained for about half the
training iterations. 


### Agenda
- 18:00 Doors open
- 18:30 - 18:45 Organizers & Host Welcome and Introduction
- 18:45 - 19:30 Diego Miguel Lozano: _I’m Feeling Lucky: The Past, Present and Future of Search_
- 19:45 - 20:30 Haris Jabbar: _MorphPiece: A Linguistic Tokenizer for Large Language Models_
- 20:30 - 22.00 Get Together With Food & Drinks


### Speakers

![Haris Jabbar ><](https://media.licdn.com/dms/image/D4E03AQFsEYPVGwXpYA/profile-displayphoto-shrink_200_200/0/1713462227514?e=1723075200&v=beta&t=dedXv0Z7XHpZO80QbPgujDCnLSHvWVprV1VN3ANqNPE)

[**Haris Jabbar**](https://www.linkedin.com/in/harisjabbar) holds Masters degrees in Computer Engineering (from Pakistan) and
Computational Science and Engineering (from TU Munich). Currently he is pursuing PhD from
LMU with a focus on inducing linguistic bias in foundation language models. This presentation is
about his work on including linguistic artifacts at tokenization level.

![Diego Miguel Lozano ><](https://media.licdn.com/dms/image/D4E03AQGmV6BFTb5sgA/profile-displayphoto-shrink_200_200/0/1712913550982?e=1723075200&v=beta&t=WyhA7k_d84EMjuhFxkFAxIUfmZa3AkEaeSkTT58drCM)

[**Diego Miguel Lozano**](https://www.linkedin.com/in/diegomiguelloz/)  – has been a NLP Engineer at KNOWRON since the early days. He is also about to complete his MSc. Informatics at TUM, with focus on NLP.