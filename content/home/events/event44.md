+++
title = "Inference Optimization with Pruna AI"
image = "/images/ev_fleischmann_pruna_ai/theme_photo.jpeg"
summary = "Speaker: Nils Fleischmann | June 17, 2025 19:00-20:00"
url = "/events/fleischmann-pruna-ai"
date = "2025-06-17"
recording = "https://www.youtube.com/watch?v=UdV6sasr_B0"
+++
<!-- https://media.licdn.com/dms/image/v2/D4E0BAQHH7NSggYcbdA/company-logo_200_200/company-logo_200_200/0/1722618537713/pruna_ai_logo?e=1754524800&v=beta&t=aNUE1LqSJQ5aIM0gq5YUYmDea6W_7z_J2UKVniZBauQ -->



{{< youtube "UdV6sasr_B0" >}}


### About this Event

Scaling has fueled the latest breakthroughs in language, image, and video models. As model sizes increase, so do the computational and energy expenses of running them. But we can do something about it.

In this talk, we'll explore compression techniques such as quantization, compilation, caching, and distillation to optimize model performance during inference. For a hands-on example, we will combine some of these techniques to reduce model size and computational load while maintaining quality, thus making AI more accessible and environmentally sustainable.

More at [Pruna AI](https://www.pruna.ai/)


### Speakers

![Nils Fleischmann ><](https://media.licdn.com/dms/image/v2/D4E03AQG7nYxbrBejeQ/profile-displayphoto-shrink_200_200/B4EZRHrmdbGgAY-/0/1736369400193?e=1753315200&v=beta&t=FEIoBn94SU9ANCL4iVwoPUorGQZCFFps4lmX-EXf6E0)

[**Nils Fleischmann**](https://www.linkedin.com/in/nilsfleischmann/) is a research engineer at Pruna AI. Pruna is a German-French Start-Up with the mission to make AI models faster, cheaper, smaller, and greener.

He primarily works on Pruna's optimization engine, which allows developers to seamlessly employ and integrate compression algorithms. He also helps customers to get the best performance out of their models.